---
title: "FinalProject_2.0"
author: "Sowmiyaa Sridharan"
date: "April 8, 2017"
output: word_document
---

```{r setup, include=FALSE}
#processed_fake <- read.csv("/Users/setaresarachi/Desktop/INFSCI 2160/DataMiningFinalProject/preprocessed_fake.csv")
processed_fake <- read.csv("preprocessed_fake.csv")
#Removing uuid and default 'x' column
processed_fake<- processed_fake[,c(-1,-3)]

#bs_count<-data.frame(table(processed_fake$type))
#colnames(processed_fake)
#k_means_data <- processed_fake[,c(3,6,7,8,9,10,11,12,13,14,16)]
#getting error
#k_means <- kmeans(k_means_data, centers = 9, nstart = 10)
#library(cluster)
#Hierachial
#Complete
#complete_unemp <- agnes(k_means_data, diss = FALSE, metric = "euclidean", method = "complete")
#plot(complete_unemp, which.plots = 2)
```

```{r PreProcessing}
#Converting domain_rank 'NA' to zero(0)
processed_fake$author <- as.character(processed_fake$author)
processed_fake$title <- as.character(processed_fake$title)
processed_fake$crawled <- as.character(processed_fake$crawled)
processed_fake$site_url <- as.character(processed_fake$site_url)

processed_fake$domain_rank[which(is.na(processed_fake$domain_rank))] <- 0

df_data <- processed_fake[,c(3,6,7,8,9,10,11,12,13,14,15,16)]
df_data <- df_data[,c(-1,-2)]

df_data$domain_rank <- as.numeric(df_data$domain_rank)
  df_data$replies_count <- as.numeric((df_data$replies_count))
  df_data$participants_count <- as.numeric((df_data$participants_count))
  df_data$comments <- as.numeric((df_data$comments))
  df_data$likes<- as.numeric((df_data$likes))
  df_data$shares <- as.numeric((df_data$shares))
```
  
```{r MultinomReg}  
library(nnet)
model_logit<-multinom(formula= type ~.,data = df_data)
```

```{r RandomForest}
library(randomForest)
rf_model <- randomForest(formula = type ~., data = df_data,ntree=100)
importance(rf_model)
str(df_data)
```

```{r kFoldRandomForest}
library(randomForest)
library(MASS) 
library(tree)
library(party)
decisionTree_matr <- as.matrix(df_data)

```

```{r 1Vs1_bait_satire}
table(df_data$type)
bait_satire <- df_data[which(df_data$type %in% c("bait", "satire")),]
bait_satire$type <- as.character(bait_satire$type)
#bait_data$type[-which(bait_data$type %in% "bait")] <- "not_bait"
table(bait_satire$type)
bait_satire$type[which(bait_satire$type %in% "bait")] <- 0
bait_satire$type[which(bait_satire$type %in% "satire")] <- 1
bait_satire$type <- as.factor(bait_satire$type)
```

```{r logit function}
lg_function <- function(training,testing){
   lg <- glm(type ~ .,data = training,family = "binomial")
   pred <- predict(lg,newdata=testing,type="response")
   return(pred)
}
```

```{r KNN}
knn_function <- function(model.mat)
#KNN model #K=5
library (class)
model.mat$random=runif(nrow(model.mat))
model.mat=model.mat[order(model.mat$random),]    # shuffle the data set
for (i in 1:nrow(model.mat))
  model.mat$group[i]=(i%%10) + 1      # make 5 groups

for (i in c(1:10))
  {
  traindata=((model.mat[model.mat$group != i, c(1:45)]))  ## traindata
  traincl=as.matrix((model.mat[model.mat$group != i, "type"]))
  test=((model.mat[model.mat$group == i, c(1:45)]))## testdata
  a = knn(traindata,            #training data as a matrix
          test,                 #testing data as a matrix
          traincl,k=5)          #class for training data
  b=data.frame(test,pred=a)
      model.mat$results[model.mat$group == i] <- b$pred  #merge predicted into model.mat
}
model.mat$results[model.mat$results == 1] <- 0
model.mat$results[model.mat$results == 2] <- 1
#Confusion Matrix
confmat <- table(model.mat$TARGET_Adjusted,model.mat$results)
names(attributes(confmat)$dimnames) <- c("Actual","Predicted")

```

```{r test}
model.mat<- model_matrix(bait_satire)
logit_result <- logit(bait_satire)

```

```{r Logit}
main <- function(data_combination){
model.mat<-model.matrix(data_combination$type~.,data=data_combination)[,-1]
model.mat[1:3, ]
model.mat <- as.data.frame(model.mat) #converting into data frame
model.mat$type <- data_combination$type 

# 10 fold cross validation for logit
for (i in 1:nrow(model.mat))
  model.mat$group[i]=(i%%10)
  result_act  = dim(1)#result_pred = dim(1)
  result_noF_act = dim(1)
  
    for (i in c(0:9)) 
      {
      training=((model.mat[model.mat$group != i, c(1:38)]))  ## traindata 
      testing=((model.mat[model.mat$group == i, c(1:38)]))  ## testdata
      #pred <- lg_function(training, testing)
      dat <- data.frame(Actual = model.mat$type[model.mat$group == i], Predicted = pred)
      btest=floor(pred+0.5) 
      dat_floored <- data.frame(Actual = model.mat$type[model.mat$group == i], Predicted = btest)
      result_noF_act=rbind(result_noF_act,dat)
      result_act=rbind(result_act,dat_floored)
      #return(result_act)
    }
      conf.mat <- table(model.mat$type,result_act$Predicted)
      names(attributes(conf.mat)$dimnames) <- c("Actual","Predicted")
      #Error
      error <- (conf.mat[2,1]+conf.mat[1,2])/nrow(model.mat)
      #Accuracy
      accuracy <- (conf.mat[1,1]+conf.mat[2,2])/nrow(model.mat)
      #Precision and Recall
      precision <- (conf.mat[1,1]) / (conf.mat[1,1]+conf.mat[2,1])
      recall <- (conf.mat[1,1]) / (conf.mat[1,1]+ conf.mat[1,2]) 
      #F-score
      f_score<- (2* precision*recall)/ (precision+recall)
      # AUC
      library(pROC)
      ROC <- roc(dat$Actual, dat$Predicted)
      #plot(ROC, col = "blue")
      AUC <- auc(ROC)
      #For ROCR Plot
      #suppressWarnings( library(ROCR))
      #rocr_lg <- prediction(result_noF_act$Predicted, model.mat$type)
      #str(rocr_lg)
      #roc_lg <- performance(rocr_lg, "sens", "fpr")
      #str(roc_lg)
      #plot(roc_lg)
      output <- c(accuracy,error,precision,recall,f_score,AUC)
      #Final Table of all values
      final_table <- data.frame(Accuracy = accuracy,Error = error, Precision = precision, Recall = recall,F_Score =          f_score, AUC = AUC )
      return(final_table)
  
}

head(result_act)
```

```{r Yuru_style}
library(e1071) # for NB and SVM
library(rpart) # for decision tree
library(ada) # for adaboost
library(class)
library(MASS) # for the example dataset 
library(plyr) # for recoding data
library(ROCR)

do.classification <- function(train.set, test.set, 
                              cl.name) {
  ## note: to plot ROC later, we want the raw probabilities,
  ## not binary decisions
  switch(cl.name, 
         knn1 = { # here we test k=1; you should evaluate different k's
           prob = knn(train.set[,-1], test.set[,-1], cl=train.set[,1], k = 1, prob=T)
           attr(prob,"prob")[prob==0] = 1-attr(prob,"prob")[prob==0] #modified
           prob = attr(prob,"prob")
           #print(cbind(prob,as.factor(test.set$Total)))
           prob
         },
         knn3 = { # here we test k=3; you should evaluate different k's
           prob = knn(train.set[,-1], test.set[,-1], cl=train.set[,1], k = 3, prob=T)
           attr(prob,"prob")[prob==0] = 1-attr(prob,"prob")[prob==0] #modified
           prob = attr(prob,"prob")
           #print(cbind(prob,as.factor(test.set$Total)))
           prob
         },
         knn5 = { # here we test k=5; you should evaluate different k's
           prob = knn(train.set[,-1], test.set[,-1], cl=train.set[,1], k = 5, prob=T)
           attr(prob,"prob")[prob==0] = 1-attr(prob,"prob")[prob==0] #modified
           prob = attr(prob,"prob")
           #print(cbind(prob,as.factor(test.set$Total)))
           prob
         },
         lr = { # logistic regression
           model = glm(y~., family=binomial, data=train.set)
          
           prob = predict(model, newdata=test.set, type="response") 
           #print(cbind(prob,as.character(test.set$y)))
           prob
         },
         nb = {
           model = naiveBayes(y~., data=train.set)
           prob = predict(model, newdata=test.set, type="raw") 
           #print(cbind(prob,as.character(test.set$y)))
           prob = prob[,2]/rowSums(prob) # renormalize the prob.
           prob
         },
         dtree = {
           model = rpart(y~., data=train.set)
                     
           prob = predict(model, newdata=test.set)
           
           if (0) { # here we use the default tree, 
             ## you should evaluate different size of tree
             ## prune the tree 
             pfit<- prune(model, cp=model$cptable[which.min(model$cptable[,"xerror"]),"CP"])
             prob = predict(pfit, newdata=test.set)
             ## plot the pruned tree 
             plot(pfit, uniform=TRUE,main="Pruned Classification Tree")
             text(pfit, use.n=TRUE, all=TRUE, cex=.8)             
           }
           #print(cbind(prob,as.character(test.set$y)))
           prob = prob[,2]/rowSums(prob) # renormalize the prob.
           #prob <- prob/sum(prob)
           prob
         },
         dtreeprune = {
           model = rpart(y~., data=train.set)
                  
           prob = predict(model, newdata=test.set)
           
           if (1) { # here we prune the tree, 
             ## you should evaluate different size of tree
             ## prune the tree 
             pfit<- prune(model, cp=model$cptable[which.min(model$cptable[,"xerror"]),"CP"])
             prob = predict(pfit, newdata=test.set)
             ## plot the pruned tree 
             plot(pfit, uniform=TRUE,main="Pruned Classification Tree")
             text(pfit, use.n=TRUE, all=TRUE, cex=.8)             
           }
           #print(cbind(prob,as.character(test.set$y)))
           prob = prob[,2]/rowSums(prob) # renormalize the prob.
           #prob <- prob/sum(prob)
           prob
         },
         svm = {
           model = svm(y~., data=train.set, probability=TRUE)
           if (0) { # fine-tune the model with different kernel and parameters
             ## evaluate the range of gamma parameter between 0.000001 and 0.1
             ## and cost parameter from 0.1 until 10
             tuned <- tune.svm(y~., data = train.set, 
                               kernel="radial", 
                               gamma = 10^(-6:-1), cost = 10^(-1:1))
             print(summary(tuned))
             gamma = tuned[['best.parameters']]$gamma
             cost = tuned[['best.parameters']]$cost
             model = svm(y~., data = train.set, probability=TRUE, 
                         kernel="radial", gamma=gamma, cost=cost)                        
           }
           prob = predict(model, newdata=test.set, probability=TRUE)
           prob = attr(prob,"probabilities")
           prob = prob[,which(colnames(prob)==1)]/rowSums(prob)
           prob
         },
         svmsig = {
           model = svm(y~., data=train.set, probability=TRUE)
           if (1) { # fine-tune the model with kernel "sigmoid"
             ## evaluate the range of gamma parameter between 0.000001 and 0.1
             ## and cost parameter from 0.1 until 10
             tuned <- tune.svm(y~., data = train.set, 
                               kernel="sigmoid", 
                               gamma = 10^(-6:-1), cost = 10^(-1:1))
             print(summary(tuned))
             gamma = tuned[['best.parameters']]$gamma
             cost = tuned[['best.parameters']]$cost
             model = svm(y~., data = train.set, probability=TRUE, 
                         kernel="radial", gamma=gamma, cost=cost)                        
           }
           prob = predict(model, newdata=test.set, probability=TRUE)
           prob = attr(prob,"probabilities")
           prob = prob[,which(colnames(prob)==1)]/rowSums(prob)
           prob
         },
         svmlin= {
           model = svm(y~., data=train.set, probability=T)
           if (1) { # fine-tune the model with kernel "linear"
             ## evaluate the range of gamma parameter between 0.000001 and 0.1
             ## and cost parameter from 0.1 until 10
             tuned <- tune.svm(y~., data = train.set, 
                               kernel="linear", 
                               gamma = 10^(-4:-1), cost = 10^(-2:1))
             #print(summary(tuned))
             gamma = tuned[['best.parameters']]$gamma
             cost = tuned[['best.parameters']]$cost
             model = svm(y~., data = train.set, probability=T, 
                         kernel="linear", gamma=gamma, cost=cost)                        
           }
           prob = predict(model, newdata=test.set, probability=T, type="raw")
           prob = attr(prob,"probabilities")
           #print(cbind(prob,as.factor(test.set$Total)))
           #print(dim(prob))
           prob = prob[,which(colnames(prob)==1)]/rowSums(prob)
           prob
         },
         ada = {
           model = ada(y~., data = train.set)
           prob = predict(model, newdata=test.set, type='probs')
           #print(cbind(prob,as.character(test.set$y)))
           prob = prob[,2]/rowSums(prob)
           prob
         }
  ) 
}

k.fold.cv <- function(dataset, cl.name, k.fold=10, prob.cutoff=0.5, get.performance=F) {
  ## default: 10-fold CV, cut-off 0.5 
  n.obs <- nrow(dataset) # no. of observations 
  s = sample(n.obs)
  errors = dim(k.fold)
  probs = NULL
  actuals = NULL
  for (k in 1:k.fold) {
    test.idx = which(s %% k.fold == (k-1) ) # use modular operator
    train.set = dataset[-test.idx,]
    test.set = dataset[test.idx,]
    prob = do.classification(train.set, test.set, cl.name)
    predicted = as.numeric(prob > prob.cutoff)
    actual = test.set$y
    confusion.matrix = table(actual,factor(predicted,levels=c(0,1)))
    confusion.matrix
    error = (confusion.matrix[1,2]+confusion.matrix[2,1]) / nrow(test.set)  
    errors[k] = error
    cat('\t\terror=',error,'\n')
    probs = c(probs,prob)
    actuals = c(actuals,actual)
    ## you may compute other measures and store them in arrays
  }
  avg.error = mean(errors)
  cat(k.fold,'-fold CV results:','avg error=',avg.error,'\n')
  
  ## plot ROC
  result = data.frame(probs,actuals)
  pred = prediction(result$probs,result$actuals)
  perf = performance(pred, "tpr","fpr")
  plot(perf)  
  
  ## get other measures by using 'performance'
  get.measure <- function(pred, measure.name='auc') {
    perf = performance(pred,measure.name)
    m <- unlist(slot(perf, "y.values"))
    #     print(slot(perf, "x.values"))
    #     print(slot(perf, "y.values"))
    m
  }
  err = mean(get.measure(pred, 'err'))
  precision = mean(get.measure(pred, 'prec'),na.rm=T)
  recall = mean(get.measure(pred, 'rec'),na.rm=T)
  fscore = mean(get.measure(pred, 'f'),na.rm=T)
  cat('error=',err,'precision=',precision,'recall=',recall,'f-score',fscore,'\n')
  auc = get.measure(pred, 'auc')
  cat('auc=',auc,'\n')
  
  if (get.performance) return (perf)
  else
    return(rbind(
      error=err,
      accuracy = 1-err,
      precision = precision,
      recall = recall,
      fscore=fscore,
      auc=auc))
}

my.classifier <- function(dataset, cl.name, get.performance=F) {
  n.obs <- nrow(dataset) # no. of observations in dataset
  n.cols <- ncol(dataset) # no. of predictors
  cat('my dataset:',
      n.obs,'observations',
      n.cols-1,'predictors','\n')
  print(dataset[1:3,])
  cat('label (y) distribution:')
  print(table(dataset$y))
  
  #pre.test(dataset, cl.name)
  k.fold.cv(dataset, cl.name, k.fold=10, prob.cutoff=0.5, get.performance=get.performance)
  #if (do.cv) k.fold.cv(dataset, cl.name, get.performance=get.performance)
}

normalize <- function(x) {
return ((x - min(x)) / (max(x) - min(x))) }

model.mat.function <- function(data_combination){
model.mat<-model.matrix(data_combination$type~.,data=data_combination)[,-1]
model.mat[1:3, ]
model.mat <- as.data.frame(model.mat) #converting into data frame
model.mat$y <- data_combination$type 
return(model.mat)
}

model.mat1 <- model.mat.function(bait_satire)
result_lr <- k.fold.cv(model.mat1,'lr', get.performance = T)
result


```


